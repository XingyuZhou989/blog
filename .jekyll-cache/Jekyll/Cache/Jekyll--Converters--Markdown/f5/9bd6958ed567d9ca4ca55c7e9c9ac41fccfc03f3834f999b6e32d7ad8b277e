I"p<p>In this series of blog posts, I plan to write down some of my personal understanding (of course shaped by many wonderful papers) of DP-FTRL and matrix matrix factorization mechanism</p>

<div class="divider"></div>

<h2 id="background">Background</h2>

<p>Building upon the classic online learning algorithm FTRL, DP-FTRL <a href="https://arxiv.org/abs/1405.7085">[KMSTTX21]</a> is the private version, proposed to get rid of subsampling and shuffling in DP-SGD while achieving similar or better privacy-utility trade-off. The key idea is simple – the regret/utility analysis in standard FTRL only relies on the prefix sum of gradients and hence it suffices to privatize each prefix sum for privacy. Not surprisingly, this idea has been explored before in private online learning, e.g., <a href="https://arxiv.org/abs/1109.0105">[JKT12]</a>, <a href="https://proceedings.neurips.cc/paper_files/paper/2013/file/c850371fda6892fbfd1c5a5b457e5777-Paper.pdf">[GS13]</a>, <a href="https://arxiv.org/pdf/1701.07953.pdf">[AS17]</a>. One popular way to privatize prefix sum is the so-called tree-based algorithm <a href="https://eprint.iacr.org/2010/076.pdf">[CSS11]</a>, <a href="https://www.wisdom.weizmann.ac.il/~naor/PAPERS/continual_observation.pdf">[DNPR10]</a> and its variance improved version by Honaker <a href="https://tpdp.journalprivacyconfidentiality.org/2015/abstracts/TPDP_2015_1.pdf">[Hon15]</a>. In particular, given a stream of gradients \(g_1, g_2,\ldots, g_T \in \mathbb{R}^d\) with bounded \(\ell_2\) norm, instead of adding noise to each gradient or to each prefix sum, the tree-based algorithm adds noise to each partial sum, see <a href="#tree">Fig. 1</a> below for an illustration.</p>

<!-- <img src="../assets/post_images/tree.jpg" alt="tree" id="tree" style="width: 90%;"/> -->

<div style="text-align: center;">
    <img src="../assets/post_images/tree.jpg" alt="tree" id="tree" style="width: 80%; height: auto;" />
</div>
<p><em>Fig 1:</em> Illustration of tree-based algorithm. It adds noise to each non-private tree node (which is a partial sum \(\sum[i,j] := \sum_{s=i}^j g_s\)). The amount of noise at each node for privacy is determined by the total sensitivity of the tree, which is \(\sqrt{O(\log T)}\) under \(\ell_2\) norm.</p>

<p>Illustration of tree-based algorithm. It adds noise to each non-private tree node (which is a partial sum \(\sum[i,j] := \sum_{s=i}^j g_s\)). The amount of noise at each node for privacy is determined by the total sensitivity of the tree, which is \(\sqrt{O(\log T)}\) under \(\ell_2\) norm.</p>

<!-- ![Image Description](URL_of_the_image){#image-ref}

![algo](../assets/post_images/alg1.jpg)(#alg) -->

<p>~\cite{jain2012differentially,guha2013nearly, agarwal2017price}. One popular way to privatize prefix sum is the so-called tree-based algorithm~\cite{chan2011private,dwork2010differential} and its variance improved version by\citet{honaker2015efficient}. Here is a quick illustration, see Fig.~\ref{fig:tree}. Given a stream of gradients $g_1, g_2,\ldots, g_T \in \Real^d$ with bounded $\ell_2$ norm, instead of adding noise to each gradient or to each prefix sum, the tree-based algorithm adds noise to each partial sum. As we will see later, these three different ways of adding noise correspond to three different factorization methods (i.e., encoder and decoder). The key benefit of adding noise to partial nodes is a good balance between sensitivity and total added noise (which is also the key aspect in matrix factorization). <a href="#image1">See the algorithm image</a></p>

<p><a id="eq1"></a>
\(\mathcal{R}(T) = \frac{1}{T} \cdot \left(\sum_{t}^T f_t(x_t) - f_t(x^*)\right) \tag{1}\)</p>

<p><a href="#eq1">See Equation (1)</a></p>

\[\mathcal{R}(T) = \frac{1}{T} \cdot \left(\sum_{t}^T f_t(x_t) - f_t(x^*)\right) \tag{1}\]

<p class="center"><strong>THE END</strong></p>

<div class="divider"></div>

<p>Now, it’s time to take a break by appreciating the masterpiece of Monet.</p>

<p class="center"><img src="../assets/post_images/cliff.jpg" alt="Monet" height="400px" width="500px" /></p>

<p class="center"><strong>Cliff Walk at Pourville</strong></p>
<p class="center"><em>courtesy of www.Claude-Monet.com</em></p>
:ET