I"_<p>In this series of blog posts, I plan to write down some of my personal understanding (of course shaped by many wonderful papers) of DP-FTRL and matrix matrix factorization mechanism</p>

<div class="divider"></div>

<p>Building upon the classic online learning algorithm FTRL, DP-FTRL <a href="https://arxiv.org/abs/1405.7085">[KMSTTX21]</a> is the private version, proposed to get rid of subsampling and shuffling in DP-SGD while achieving similar or better privacy-utility trade-off. The key idea is simple – the regret/utility analysis in standard FTRL only relies on the prefix sum of gradients and hence it suffices to privatize each prefix sum for privacy. Not surprisingly, this idea has been explored before in private online learning, e.g., <a href="https://arxiv.org/abs/1109.0105">[JKT12]</a>, <a href="https://proceedings.neurips.cc/paper_files/paper/2013/file/c850371fda6892fbfd1c5a5b457e5777-Paper.pdf">[GS13]</a>, <a href="https://arxiv.org/pdf/1701.07953.pdf">[AS17]</a>. One popular way to privatize prefix sum is the so-called tree-based algorithm <a href="https://eprint.iacr.org/2010/076.pdf">[CSS11]</a>, <a href="https://www.wisdom.weizmann.ac.il/~naor/PAPERS/continual_observation.pdf">[DNPR10]</a> and its variance improved version by Honaker <a href="https://tpdp.journalprivacyconfidentiality.org/2015/abstracts/TPDP_2015_1.pdf">[Hon15]</a>. In particular, given a stream of gradients \(g_1, g_2,\ldots, g_T \in \mathbb{R}^d\) with bounded \(\ell_2\) norm, instead of adding noise to each gradient or to each prefix sum, the tree-based algorithm adds noise to each partial sum, see <a href="#tree">Fig. 1</a> below for an illustration.</p>

<div style="text-align: center;">
    <img src="../assets/post_images/tree.jpg" alt="tree" id="tree" style="width: 80%; height: auto;" />
</div>
<p><em>Fig 1:</em> Illustration of tree-based algorithm. It adds noise to each non-private tree node (which is a partial sum \(\sum[i,j] := \sum_{s=i}^j g_s\)). The amount of noise at each node for privacy is determined by the total sensitivity of the tree, which is \(\sqrt{O(\log T)}\) under \(\ell_2\) norm.  Each noisy prefix sum at time \(t\) can be computed by at most \(O(\log t)\) noisy tree nodes. Thus, the total additional noise variance is \(O_{\delta}(\log t \cdot \log T\cdot 1/\epsilon^2)\) for \((\epsilon,\delta)\)-DP. Ignoring all log terms, we have the total noise in noisy prefix sum is \(\tilde{O}_{\delta}(1/\epsilon^2)\) for each coordinate \(i\in [d]\). Note that, for the online update, one only needs to compute the noisy tree nodes indicated by green color (see <a href="https://xingyuzhou.org/talks/FLCB-OSU.pdf">my slides</a> (page 29) for details).</p>

<div class="divider"></div>

<p>DP-FTRL essentially leverages the idea of tree-based algorithm to achieve a similar or better privacy-utility trade-off than DP-SGD, without requiring the use of privacy amplification by subsampling or shuffling. One utility metric considered in <a href="https://arxiv.org/abs/1405.7085">[KMSTTX21]</a> is online (average) regret defined below.</p>

<p><a id="eq1"></a>
\(\mathcal{R}(T) = \frac{1}{T} \cdot \left(\sum_{t}^T f_t(x_t) - f_t(x^*)\right), \tag{1}\)
where \(f_t(x) = \ell (x; \xi_t)\) and \(\ell(x;\xi_t)\) is the (convex) loss of model parameter $x$ under sample \(\xi_t\) and a fixed loss function \(\ell\). \(x^*\) is any reference parameter.  As in standard online learning, $f_t$ can be adversarially chosen (hence \(\xi_t\) can be arbitrarily chosen without distribution assumption).</p>

<p>~\cite{jain2012differentially,guha2013nearly, agarwal2017price}. One popular way to privatize prefix sum is the so-called tree-based algorithm~\cite{chan2011private,dwork2010differential} and its variance improved version by\citet{honaker2015efficient}. Here is a quick illustration, see Fig.~\ref{fig:tree}. Given a stream of gradients $g_1, g_2,\ldots, g_T \in \Real^d$ with bounded $\ell_2$ norm, instead of adding noise to each gradient or to each prefix sum, the tree-based algorithm adds noise to each partial sum. As we will see later, these three different ways of adding noise correspond to three different factorization methods (i.e., encoder and decoder). The key benefit of adding noise to partial nodes is a good balance between sensitivity and total added noise (which is also the key aspect in matrix factorization). <a href="#image1">See the algorithm image</a></p>

<p><a href="#eq1">See Equation (1)</a></p>

\[\mathcal{R}(T) = \frac{1}{T} \cdot \left(\sum_{t}^T f_t(x_t) - f_t(x^*)\right) \tag{1}\]

<p class="center"><strong>THE END</strong></p>

<div class="divider"></div>

<p>Now, it’s time to take a break by appreciating the masterpiece of Monet.</p>

<p class="center"><img src="../assets/post_images/cliff.jpg" alt="Monet" height="400px" width="500px" /></p>

<p class="center"><strong>Cliff Walk at Pourville</strong></p>
<p class="center"><em>courtesy of www.Claude-Monet.com</em></p>
:ET