I" 6\[\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}

\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}\]

<p>In this series of blog posts, I plan to write down some of my personal understanding (of course shaped by many wonderful papers) of DP-FTRL and matrix matrix factorization mechanism.</p>

<div class="divider"></div>

<p>As motivated in the end of the <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(II)">previous post</a>, in this post, our goal is to give a general way to compute the total noise (error) in prefix sum for any factorization \(\bA = \bB\bC\). As a result, one can then optimize this error (objective) under privacy constraints to search for an even better mechanism than the tree-based algorithm (i.e., better pair \((\bB, \bC)\)). From this, we will see that one needs to be careful in choosing \(\bC\) and \(\bB\) to balance the (i) sensitivity (hence the variance for each noise, determined by \(\bC\)) and (ii) the total number of accumulated noise (determined by \(\bB\)). The key behind tree-based algorithm is that its pair of \((\bC, \bB)\) guarantees that both (i) and (ii) are on the log order, which is in sharp contrast to Simple I and II.</p>

<p>The calculation for the error in the matrix factorization mechanism is standard ( e.g.,<a href="https://people.cs.umass.edu/~miklau/assets/pubs/dp/Li15matrix.pdf">[LMHMR15]</a>). Here, we first slightly deviate from existing works by explicitly focusing on the task of prefix sum and aiming to bound the maximal error with high probability rather than expectation. Another difference is that for clarity,  we explicitly consider the fact that the input data \(\bG\) is a \(T \times d\) matrix rather than \(T \times 1\) vector in standard previous works.  Later, we will give a short remark on the general way to bound the <em>expected</em> error. To start with, we can easily see that the max error in \(T\) prefix outputs (i.e., \(\bA\) is the all-ones lower triangular matrix) is given by</p>

\[\text{Max-Error}(\bB,\bC) := \max_{i \in [T]} \norm{\bB_{[i,:]} \bZ}_2,\]

<p>where \(\bB_{[i,:]}\) is the \(i\)-th row of \(\bB\) and each element of \(\bZ\) is \(i.i.d\) sampled from \(\cN(0,\sigma^2)\), which is used to provide \((\epsilon,\delta)\)-DP. To this end, \(\sigma = \sigma_{\epsilon,\delta} \cdot \mathrm{sens}(\bC)\) (assume \(\ell_2\) norm of \(g_t\) is bounded by \(1\) again) with \(\sigma_{\epsilon,\delta} := O\left(\frac{\sqrt{\log(1/\delta)}}{\epsilon}\right)\) and \(\mathrm{sens}(\bC)\) is the \(\ell_2\)-sensitivity of \(\bC \bG\) (here it is Frobenious norm since \(\bC \bG\) is a matrix) <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. Let us first compute the high-probability max-error for a general \(\sigma\) and then plug in the proper value of it for privacy. For each \(i\), we have each element of the \(d\)-dimensional row vector \(\bB_{[i,:]} \bZ\) is distributed according to \(\cN(0, \norm{\bB_{[i,:]}}_2^2)\). Hence, by the standard concentration of Gaussian vector and union bound over all \(i\), we have with high probability</p>

\[\label{eq:max-error}
    \text{Max-Error}(\bB,\bC) \le  \max_{i \in [T]} O\left(\sigma \cdot \sqrt{d} \cdot \norm{\bB_{[i,:]}}_2\right).\]

<p>It remains to determine the value of \(\sigma\), i.e.,  \(\mathrm{sens}(\bC)\). To this end, we can write \(\bC \bG\) as a linear combination of the columns in \(\bC\)</p>

\[\bC \bG = \bC_{[:,1]} g_1^{\top}  + \cdots \cdot \bC_{[:,j]} g_j^{\top} + \cdots + \cdot \bC_{[:,T]} g_T^{\top},\]

<div class="divider"></div>

<p>, we would like to explore even better mechanisms than tree-based algorithm for the task of releasing private prefix sum in this post. The key idea here is to view the tree-based algorithm (as well as the other two simple noise-adding mechanisms mentioned in the beginning of <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(I)">previous post</a>) as special cases of a more general framework â€“ matrix factorization mechanism <a href="https://arxiv.org/pdf/2202.08312.pdf">[DMRSG22]</a>. In particular, one key contribution of <a href="https://arxiv.org/pdf/2202.08312.pdf">[DMRSG22]</a> is to explicitly consider the stream nature and privacy under <em>adaptive</em> inputs, which is in contrast to previous offline settings (e.g.,<a href="https://people.cs.umass.edu/~miklau/assets/pubs/dp/Li15matrix.pdf">[LMHMR15]</a>, <a href="https://arxiv.org/pdf/1911.08339.pdf">[ENU20]</a>). Following <a href="https://arxiv.org/pdf/2202.08312.pdf">[DMRSG22]</a>, let us have a quick understanding of the matrix factorization mechanism, with a focus on the task of computing prefix sum. Note that it can be easily generalized to general tasks (with replacement of the task matrix \(\bA\) below).</p>

<p>The task of online releasing prefix sum over a sequence of gradients \(\bG = [g_1,\ldots, g_T]^{\top} \in \Real^{T \times d}\) can be represented as \(\bA \bG\) where \(\bA \in \Real^{T \times T}\) is the all-ones lower-triangular matrix. To privatize \(\bA \bG\), the matrix factorization approach first factorizes \(\bA = \bB \bC\) and releases \(\bB (\bC \bG + \bZ)\), where \(\bZ \in \Real^{T \times d}\) is some proper privacy noise matrix. By post-processing of DP, it suffices to ensure that \(\bC \bG + \bZ\) is private (although one needs to be careful to deal with adaptive inputs).
The high-level intuition behind this method is to adjust the space where one adds noise (hence \(\bC\) is often called the <em>encoder</em>) and then reconstruct the required output using the <em>decoder</em> matrix \(\bB\). The hope here is to achieve a better privacy and utility (e.g., total noise in noisy prefix sum) trade-off. As we will see later, roughly speaking, \(\bC\) controls the variance for each added noise while \(\bB\) controls the number of noise that will be added to each noisy prefix sum.</p>

<div class="divider"></div>

<p>Before searching for better mechanisms, let us first cast the tree-based algorithm and the other two mechanism to the matrix factorization framework with different choices of \((\bB,\bC)\). From this, we can see that indeed a careful choice of \((\bB,\bC)\) can improve the performance.</p>

<p><strong>Example 1 (Simple I mechanism as matrix factorization)</strong> Simple I mechanism in <a href="https://eprint.iacr.org/2010/076.pdf">[CSS11]</a> simply adds noise to each non-private prefix sum. This corresponds to \(\bB = \bI\) and \(\bC = \bA\). For privacy, the sensitivity is on the order of \(\sqrt{T}\) in \(\ell_2\) norm as changing the first element \(g_1\) will impact all \(T\) outputs in the space of \(\bC \bG = \bA \bG\). Hence, each element in \(\bZ\) is \(\cN(0,\sigma^2)\) with \(\sigma^2 \approx O_{\delta}\left(\frac{T}{\epsilon^2}\right)\) for \((\epsilon,\delta)\)-DP.  For utility, each noisy prefix sum is simply the non-private prefix sum plus the \(t\)-th row vector \(z_t\) in \(\bZ\). Putting the two together, we have the total noise in the noisy prefix sum is \(O_{\delta}(T/\epsilon^2)\) for \((\epsilon,\delta)\)-DP.</p>

<p><strong>Example 2 (Simple II mechanism as matrix factorization)</strong> Simple II mechanism in <a href="https://eprint.iacr.org/2010/076.pdf">[CSS11]</a> simply adds noise to each input data point and then accumulates all noisy data points for noisy prefix sum. This corresponds to \(\bB = \bA\) and \(\bC = \bI\). For privacy, the sensitivity is on the order of \(O(1)\) in \(\ell_2\) norm as changing any element \(g_t\) will at most change \(O(1)\) in the output.  Hence, each element in \(\bZ\) is \(\cN(0,\sigma^2)\) with \(\sigma^2 \approx O_{\delta}\left(\frac{1}{\epsilon^2}\right)\) for \((\epsilon,\delta)\)-DP.  For utility, each noisy prefix sum needs to accumulate all previous noisy data points. Putting the two together, we have the total noise in the noisy prefix sum is again \(O_{\delta}(T/\epsilon^2)\) for \((\epsilon,\delta)\)-DP, the same as above.</p>

<p><strong>Example 3 (Tree-based algorithm as matrix factorization)</strong>    The tree-based algorithm adds noise to the tree nodes in the complete binary tree as shown in Fig.1 of the <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(I)">previous post</a> and then accumulates the corresponding noisy tree nodes for the final noisy prefix sum. This can also be viewed as a proper choice of \(\bB\) and \(\bC\). Let us first see a concrete toy example for the simple case when \(T = 4\) and then give results for the general case later. In particular, for \(T = 4\), we have</p>

\[\bC = \begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 &amp; 1
\end{pmatrix}, \quad
\bB = \begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}.\]

<p>To see it, as shown in the left sub-tree of Fig.1 of the <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(I)">previous post</a>, the encoder matrix \(\bC \in \Real^{7 \times 4}\) is responsible for computing the \(7\) tree nodes (partial sums) using \(4\) input data points. On the other hand, the decoder \(\bB \in \Real^{4 \times 7}\) is responsible for computing the noisy prefix sum using the \(7\) noisy tree nodes. One can check \(\bA = \bB \bC\). For the general case, by the recursive nature of a binary tree, it is very natural to conjecture that \(\bC\) would enjoy some recursive formula. Indeed, as shown in <a href="https://arxiv.org/pdf/2202.08312.pdf">[DMRSG22]</a>, if one defines the following recursive formula</p>

\[\bH_1 = \begin{pmatrix}
1
\end{pmatrix}, \quad
\bH_{k+1} = \begin{pmatrix}
\bH_k &amp; \mathbf{0}  \\
0 &amp; \bH_k \\
\mathbf{1} &amp; \mathbf{1}
\end{pmatrix},\]

<p>then, for \(T =2^{k-1}\), \(k \ge 1\), \(\bC = \bH_k\) and \(\bB\) is a \(\{0,1\}\)-valued matrix such that \(\bB \bC = \bA\) which can be found using tree structure (or bit representation of \(t\)) or using linear programming. As discussed in Fig.1 of the <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(I)">previous post</a>, the total noise in noisy prefix sum under tree-based algorithm is only \(\tilde{O}_{\delta}(1/\epsilon^2)\).</p>

<p><em>Remark 1:</em> Note that due to the constraint of online streaming release, \(\bB\) in the tree-based algorithm cannot be the one computed via \(\bA \bC^{\dagger}\) where \(\bC^{\dagger}\) is the Moore-Penrose pseudoinverse of matrix \(\bC\). Interestingly, as shown in <a href="https://arxiv.org/pdf/2202.08312.pdf">[DMRSG22]</a> (cf. Proposition C.1), \(\bA \bC^{\dagger}\) is exactly the (non-streaming) Honaker fully efficient estimator in <a href="https://tpdp.journalprivacyconfidentiality.org/2015/abstracts/TPDP_2015_1.pdf">[Hon15]</a>. For general \(T=2^{k-1}\), please refer to my <a href="https://colab.research.google.com/drive/1B0mN9rQOZ-u8Ox9ZjNYkYpjBf187fcqL?usp=sharing">Python code</a> for computing \(\bB\) for the tree-based algorithm. BTW, we can see that the all-zero columns in \(\bB\) responds to the non-green nodes in Fig.1 of the <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(I)">previous post</a>.</p>

<p><em>Remark 2:</em> For approximate DP, Simple I and Simple II mechanisms have similar amounts of total noise as demonstrated above. However, if one considers pure DP with Laplace noise, one can even distinguish between the two mechanisms. In particular, for Simple I, the variance of the error in each prefix sum is one noise with variance of \(O(T^2/\epsilon^2)\) (since now the sensitivity is in \(\ell_1\) norm). On the other hand, for Simple II, the corresponding value is the sum of at  most \(O(T)\) noise, each with variance of \(O(1/\epsilon^2)\). This amounts to a total variance of \(O(T/\epsilon^2)\), hence \(T\) factor smaller.</p>

<div class="divider"></div>

<p>Thus, we have seen that there indeed exists the possibility of optimizing the choice of \((\bB,\bC)\) for a better privacy-utility trade-off in terms of total noise. In the next post, we will come up with a loss objective for the maximal total error in prefix sum so that we can use optimization to find the best \((\bB,\bC)\). But, we should also keep in mind, whether or not minimizing the maximal total noise in prefix sum leads to a tight final performance, as hinted in Q2 of the <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(I)">previous post</a>, which will be our later topic.</p>

<p class="center"><strong>THE END</strong></p>

<div class="divider"></div>

<p>Now, itâ€™s time to take a break by appreciating the masterpiece of Monet.</p>

<p class="center"><img src="../assets/post_images/water-lilies-2.jpg" alt="Monet" height="400px" width="500px" /></p>

<p class="center"><strong>Water Lilies</strong></p>
<p class="center"><em>courtesy of https://www.wikiart.org/</em></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>One subtlety here is that gradients in \(\bG\) in model training is adaptive rather than fixed. However, as shown in <a href="https://arxiv.org/pdf/2202.08312.pdf">[DMRSG22]</a>, for the Gaussian mechanism, it suffices to consider the non-adaptive one.Â <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET