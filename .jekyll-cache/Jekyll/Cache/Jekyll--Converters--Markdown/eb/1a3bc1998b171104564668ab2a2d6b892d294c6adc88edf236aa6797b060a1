I"¦\[\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cR}{\mathcal{R}}

\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\ex}[1]{\mathbb{E}\left[ #1 \right] }
\newcommand{\cL}{\mathcal{L}}\]

<p>In this series of blog posts, I plan to write down some of my personal understanding (of course shaped by many wonderful papers) of DP-FTRL and matrix factorization mechanism.</p>

<div class="divider"></div>

<p>As motivated at the end of the <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(III)">previous post</a>, in this post, we aim to study whether minimizing the error in prefix sum is sufficient for a tight performance guarantee. To this end, we first relate DP-FTRL with DP-SGD, from which we will see that the FTRL-style analysis (along with its regret, i.e., <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(I)#eq2">(1.2)</a> is not tight in the DP case. Instead, as first observed in <a href="https://arxiv.org/abs/2302.01463">[KMCRM23]</a>, we might need to carefully analyze the <em>per-step</em> behavior rather than only focusing on the noisy prefix sum (this might remind you about the standard FTRL vs. OMD (OGD) style analysis in online learning). In the following, let us first use a simple example to show that the prefix-sum view and its analysis in <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(I)">(I)</a> is not tight, which in turn questions the optimization objective in the matrix factorization of <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(III)">(III)</a>.</p>

<p><strong>Example 4.1 (One-pass DP-SGD without Amplification)</strong> Under this algorithm denoted by \(\cA\), one Gaussian noise \(z_t\) is added to each stepâ€™s gradient, which is computed on the \(t\)-th data sample.  For this algorithm, we will show that FTRL-style analysis as in <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(I)">(I)</a> only gives a loose bound while per-step OGD-style analysis gives a tighter bound. First, under FTRL-style analysis, \(\cA\) just replaces tree-based algorithm in DP-FTRL with Simple II (cf. Example 2.2 in <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(II)">(II)</a>). Thus, by Example 2.2, we have the maximal error in <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(I)#eq2">(1.2)</a> is on the order of \(\sqrt{T}/\epsilon\) (ignoring \(d\)). Choosing \(\lambda\) optimally gives the privacy cost of \(O(1/T^{1/4})\) in regret. On the other hand, by a simple adaptation of standard OGD (adding additional noise \(O(1/\epsilon^2)\) to each gradient), the privacy cost is only \(O(1/(\epsilon \sqrt{T}))\) in regret.</p>

<div class="divider"></div>

<p>Therefore, one question to ask is â€“ can we formulate DP-FTRL with different mechanisms in a similar style to DP-SGD? Then, one can try to adapt the standard OGD-style (per-step) analysis to have a tighter bound. This is exactly the main motivation of the recent nice NeurIPS paper <a href="https://arxiv.org/abs/2302.01463">[KMCRM23]</a>. In the following, let us first look at another view of DP-FTRL (with tree-based algorithm), as illustrated in the figure below.</p>

<div style="text-align: center;">
    <img src="../assets/post_images/correlated-noise.jpg" alt="tree" id="correlated" style="width: 80%; height: auto;" />
</div>
<p><em>Fig 4.1:</em> DP-SGD vs. DP-FTRL. We compare the per-step information in each iteration step \(t\). For DP-SGD, the non-private gradient \(g_t\) is perturbed by <em>independent</em> noise \(z_t\). On the other hand, for DP-FTRL, the non-private gradient \(g_t'\) is perturbed by <em>negatively correlated</em> noise. One thing to note is that the non-private gradients are not the same (\(g_t\) vs. \(g_t'\)) even if it is computed on the same sample for each \(t\), due to the different model updates under DP-SGD and DP-FTRL.</p>

<p>In <a href="https://arxiv.org/abs/2302.01463">[KMCRM23]</a>, the authors write all MF-DP-FTRL (i.e., DP-FTRL with general matrix factorization) as a similar update in <a href="#correlated">Fig. 4.1</a>. In particular, with \(\bA = \bB \bC\), the per-step new information is given by \((\bB_{[t+1,:]} - \bB_{[t,:]}) \bZ\) (check it using <a href="#correlated">Fig. 4.1</a> and <a href="https://xingyuzhou.org/blog/notes/DP-FTRL-and-matrix-factorization-(II)#eq21">(2.1)</a>. Hence, the iterate update is given by
\(x_{t+1} = x_t -\eta \cdot \left(g_t + (\bB_{[t+1,:]} - \bB_{[t,:]}) \bZ \right).\)
Based on this, one key contribution of <a href="https://arxiv.org/abs/2302.01463">[KMCRM23]</a> is to directly analyze its convergence, which is tighter than the prefix-sum view results in the previous two sections. Moreover, with its new analysis, it recovers the \(O(1/\sqrt{T})\) for the algorithm in Example 4.1 above (cf. Theorem 4.7 in <a href="https://arxiv.org/abs/2302.01463">[KMCRM23]</a>).</p>

<p>Let us end this section with another simple algorithm that can achieve \(O(1/\sqrt{T\epsilon})\) regret as DP-FTRL with tree-based algorithm for \(\epsilon \le 1\). In fact, this new algorithm is even better in terms of log terms.</p>

<p><strong>Example 4.1 (Batch OGD)</strong> Under this algorithm, it only updates the model every \(B\) iterations. For each batch \(m \in [T/B]\), it computes the gradient using the most recent model parameter, takes the average of the new batch of gradients, and adds noise for privacy. This is essentially running the algorithm in Example 4.1 above with only \(M:= T/B\) steps and the noise for each step is \(1/B\) factor smaller due to averaging. Thus, the total regret under \((\epsilon,\delta)\)-DP can be written as</p>

\[\cR_{\text{batch-OGD}}(T) \le O_{\delta}\left(B \cdot \left(\frac{1}{\sqrt{M}} + \frac{1}{B \epsilon  \sqrt{M}}\right)\right).\]

<p>Optimizing the choice of \(B = 1/\epsilon\), yields regret as \(O_{\delta}\left(\frac{1}{\epsilon \sqrt{T}}\right)\).</p>

<div class="divider"></div>

<p><strong>Update Sep. 23, 2024:</strong> While attending TPDP this year, <a href="https://cseweb.ucsd.edu/~yuxiangw/">Yu-Xiang Wang</a> mentioned a very nice view that puts DP-FTRL as an intermediate point between two extreme points (see also Section I.2 in Rachel Redbergâ€™s <a href="https://arxiv.org/pdf/2401.00583">paper</a>). One end point in the spectrum is DP-SGD where i.i.d noise is added at each iteration, while the other end point is objective perturbation where fully correlated noise is added (i.e., identical noise). Thus, DP-FTRL is an intermediate point where some degree of correlation in the noise exists (as shown in <a href="#correlated">Fig. 4.1</a>). As suggested by Yu-Xiang, given that both two points (DP-SGD and objective perturbation) can achieve a good performance (even optimal), it gives us hope that DP-FTRL can also achieve an optimal performance.</p>

<p class="center"><strong>THE END</strong></p>

<div class="divider"></div>

<p>Now, itâ€™s time to take a break by appreciating the masterpiece of Monet.</p>

<p class="center"><img src="../assets/post_images/water-lilies-4.jpg" alt="Monet" height="400px" width="500px" /></p>

<p class="center"><strong>Water Lilies</strong></p>
<p class="center"><em>courtesy of https://www.wikiart.org/</em></p>
:ET