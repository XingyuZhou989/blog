I"C<p>Recently, I have been interested in privacy protection in online decision-making, with a focus on bandits and reinforcement learning (RL). In this post, I would like to point out some subtlety in the definition of differential privacy (DP) for bandits and RL.</p>

<p>Let us start with the simple multi-armed bandit learning protocol.</p>

<p><img src="../assets/post_images/alg1.jpg" alt="algo" /></p>

<p>In practice, the reward feedback is often sensitive (e.g., medical treatment). Thus, most existing papers on private bandits adapt the standard central model of DP to bandit learning as follows.</p>

<p><img src="../assets/post_images/def1.jpg" alt="def" /></p>

<p>However, this definition is not satisfying and also problematic in some sense:</p>

<ul>
  <li>
    <p>First,  what we really want to protect is the identity of the user rather than a simple reward feedback at a particular round. More specifically, we want to guarantee that any adversary who observes the agent‚Äôs prescribed actions cannot infer too much about the sensitive information of any user. The sensitive information here could be whether the user has participated in the learning process or not, as well as her <strong>entire preference (reward feedback) for all possible prescribed actions</strong> ‚Äì which defines the ‚Äútype‚Äù of the user.</p>
  </li>
  <li>
    <p>Second, the above definition implicitly assumes that the learning agent can be fed with two reward sequences that only differ in one round. However, this is somewhat problematic since the change of reward feedback in round \(t\), will naturally  change the agent‚Äôs algorithm for <strong>future rounds</strong>, hence different reward feedback after round \(t\).</p>
  </li>
</ul>

<div class="divider"></div>

<p>To address the above issues, a more proper definition is to adopt users as the dataset, i.e., \(D = (u_1,\ldots, u_T)\). Technically speaking, we  say two users are identical if they have the same preference (reward feedback) for all possible actions, i.e., they are the same ‚Äútype‚Äù. Then, we have the following definition.</p>

<p><img src="../assets/post_images/def2.jpg" alt="def2" /></p>

<p>By definition, we can see that using the users as the dataset directly protects the identity of each user, hence resolving the first issue above. Moreover, now we can replace one user while fixing all other users the same, hence fixing the second issue above.</p>

<p>The above definition is not the so-called user-level DP. It is still an item-level DP. Let‚Äôs compare it with standard item-level DP for private SGD (e.g., <a href="https://arxiv.org/abs/1405.7085">[BST‚Äô14]</a>), where each data point is (feature, label), which will generate feedback (e.g., gradient) at a particular parameter weight vector. This is similar to bandit learning when the dataset consists of users. That is, each data point is a user (i.e., the entire reward preference), which will generate feedback (e.g., reward \(r_t\)) upon receiving a particular action. This in turn justifies that Def. 2 is a proper and more natural one. In fact, private online learning with full information (rather than bandit feedback) also considers the entire loss vector as the datapoint in \(\mathcal{D}\) (see <a href="https://proceedings.neurips.cc/paper/2013/hash/c850371fda6892fbfd1c5a5b457e5777-Abstract.html">[ST‚Äô13]</a>), which shares the same spirit as Def. 2.</p>

<div class="divider"></div>

<p>Now, one can easily see that the second definition is stronger than the first one <em>in general</em>. Then, one may wonder <em>whether the current private bandit algorithm that guarantees DP under Def. 1 still provides DP guarantee under Def. 2</em>. In particular, if one user \(u_t\) is changed, it will not only affect the reward feedback at round \(t\), but it will also change future feedback. Will this cause problems? Note that under Def.1, it assumes that there is only one feedback that is different.</p>

<p>It turns out that it will NOT be a problem. That is, existing privacy mechanism in private bandit learning papers still provides DP guarantee under Def.2 thanks to <strong>adaptive</strong> composition of DP. This property is in fact already used in private SGD or private online learning to address a similar issue, where changing one sample or loss vector will impact all follow-up gradient feedback (e.g., see the proof of Theorem 3 in <a href="https://proceedings.neurips.cc/paper/2013/hash/c850371fda6892fbfd1c5a5b457e5777-Abstract.html">[ST‚Äô13]</a> where the authors explicitly mentioned how to address it using adaptive composition). The key intuition is that for a given output event \(\mathcal{E}\), one computes its probability using chain rule of conditional probability. This conditioning on the same action removes the possible correlation. The same intuition also applies to the computation of privacy loss random variable, which is widely used in establishing adaptive composition. Note that adaptive composition also holds for other types of DP, e.g., zero-concentrated DP <a href="https://arxiv.org/abs/1605.02065">[BS‚Äô16]</a> and Renyi DP <a href="https://arxiv.org/abs/1702.07476">[Mironov‚Äô17]</a></p>

<p>For episodic reinforcement learning (RL), one can easily follow the same fashion by considering dataset that consists of users. Now, the ‚Äútype‚Äù of a user is identified by the state and reward she would give to all possible \(A^H\) actions, where \(A\) is the number of actions and \(H\) is the length of one episode. This is exactly the definition adopted in <a href="https://arxiv.org/abs/2009.09052">[VBKW‚Äô20]</a> (where when \(H=1\) and number of states \(S=1\), it reduces to Def. 2 above for MAB).  Note that in RL, the output sequence needs to be slightly modified by considering joint DP, i.e., removing the requirement for her own action. Otherwise, one has to incur linear regret (which is also easy to understand since now action is personalized with the state).</p>

<h3 id="summary">Summary</h3>
<ul>
  <li>
    <p>One commonly used DP definition for bandits is not satisfying.</p>
  </li>
  <li>
    <p>A more proper one is to consider dataset that consists of users (rather than the particular observation)</p>
  </li>
  <li>
    <p>Thanks to <strong>adaptive</strong> composition of DP, the existing privacy mechanism still provides formal privacy guarantee under the new definition.</p>
  </li>
</ul>

<h3 id="one-more-thing">One more thing‚Ä¶</h3>

<p>If you are also interested in private bandit or RL, please take a look at our recent publications (with <a href="https://scholar.google.com/citations?user=Q0_CaxYAAAAJ&amp;hl=en">Sayak Ray Chowdhury</a>) üòä. In particular, see our recent <a href="https://arxiv.org/pdf/2206.05772.pdf">ICLR‚Äô23</a> paper for private bandit<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, <a href="https://arxiv.org/abs/2202.05567">ICML‚Äô22</a> for private contextual bandits and <a href="https://arxiv.org/abs/2112.10599">AAAI‚Äô22</a> for private RL. All of them provide a comprehensive study of different trust models of DP, e.g., central, local and distributed DP. In the future, I may also try to give more details on each paper.</p>

<!--
Last time, we have mentioned that there exists a duality between [strong convexity](http://xingyuzhou.org/blog/notes/strong-convexity) and [Lipschitz continuous gradient](http://xingyuzhou.org/blog/notes/Lipschitz-gradient). In this post, we will explore this duality, which is often called _Fenchel duality_.

This duality actually relates to the convex conjugate of a function. Thus, to begin with, we will first introduce the definition of conjugate function and some useful results.

The conjugate of a function $$f$$ is

$$		
	f^*(s) = \sup_{x \in \text{dom} f} (s^T x - f(x))
$$

Now, we will introduce several useful results on conjugate function.

>**Lemma** _Consider the following conditions:_
>
$$
\begin{align*}
	[1]~& f^*(s) =  s^Tx - f(x)\\
	[2]~& s \in \partial f(x)\\
	[3]~& x \in \partial f^*(s)
\end{align*}
$$
>
Then we have the following results:
>
(i) For a general $$f$$, we have [1] $$\iff$$ [2] $$\implies$$ [3].
>
(ii) For a closed and convex $$f$$, we have [1]-[3] are all equivalent.
>
(iii) For a closed and strictly convex $$f$$, we have $$\nabla f^*(s) = \arg\!\max_x(s^T x - f(x))$$.

_Proof:_ Please refer to my notes [here](http://xingyuzhou.org/talks/Fenchel_duality.pdf) (page 40).

<div class="divider"></div>

Now, we are well prepared to prove the following theorem. The proof is quite simple once you have a good understanding of my previous posts on strong convexity and Lipschitz continuous gradient.

>**Theorem**
>
(i) _If $$f$$ is closed and strong convex with parameter $$\mu$$, then $$f^*$$ has a Lipschitz continuous gradient with parameter $$\frac{1}{\mu}$$_.
>
(ii) _If $$f$$ is convex and has a Lipschitz continuous gradient with parameter $$L$$, then $$f^*$$ is strong convex with parameter $$\frac{1}{L}$$_.

_Proof:_

(1): By implication of strong convexity shown in the previous post on [strong convexity](http://xingyuzhou.org/blog/notes/strong-convexity), we have

$$
\begin{align*}
		\lVert s_x - s_y \rVert \ge \mu\lVert x-y \rVert ~\forall s_x\in \partial f(x), s_y\in \partial f(y)
\end{align*}
$$

Based on the previous lemma, we have

$$
\begin{align*}
		\lVert s_x - s_y\rVert \ge \mu\lVert \nabla f^*(s_x)- \nabla f^*(s_y) \rVert
\end{align*}
$$

Hence, $$f^*$$ has a Lipschitz continuous gradient with $$\frac{1}{\mu}$$.


(2): By implication of Lipschitz continuous gradient for convex $$f$$ shown in the post of [Lipschitz continuous gradient](http://xingyuzhou.org/blog/notes/Lipschitz-gradient), we have

$$
\begin{align*}
		(\nabla f(x) - \nabla f(y)^T(x-y) \ge \frac{1}{L} \lVert \nabla f(x)-\nabla f(y)\rVert^2
\end{align*}
$$

which implies based on the previous lemma

$$
\begin{align*}
		(s_x - s_y)^T (x-y) \ge \frac{1}{L} \lVert s_x - s_y \rVert^2~\forall x \in \partial f^*(s_x), y \in \partial f^*(s_y)
\end{align*}
$$

Hence, $$f^*$$ is strongly convex with parameter $$\frac{1}{L}$$ according to the previous post on [strong convexity](http://xingyuzhou.org/blog/notes/strong-convexity). $$\tag*{$\Box$}$$ -->

<!-- Last time, we talked about [strong convexity](http://xingyuzhou.org/blog/notes/strong-convexity). Today, let us look at another important concept in convex optimization, named _Lipschitz continuous gradient_ condition, which is essential to ensuring convergence of many gradient decent based algorithms. The post is also mainly based on my course project report.  

It is worth noting that there exits a duality (Fenchel duality) between strong convexity and Lipschitz continuous gradient, which implies that once we have a good understanding of one, we may easily understand the other one.

**Note:** Indeed, all the results in this post can be easily proved via the same method adopted in the post of strong convexity. This is the beauty of duality!

As usual, let's us first begin with the definition.

A differentiable function $$f$$ is said to have an L-Lipschitz continuous gradient if for some $$L>0$$

$$
\lVert \nabla f(x) - \nabla f(y)\rVert \le L \lVert x-y\rVert,~\forall x,y.
$$

**Note:** The definition doesn't assume convexity of $$f$$.

Now, we will list some other conditions that are related or equivalent to Lipschitz continuous gradient condition.

$$
\begin{align}
	[0]~&\lVert\nabla f(x) - \nabla f(y)\rVert \le L \lVert x-y\rVert,~\forall x,y.\\
	[1]~&g(x) = \frac{L}{2}x^T x - f(x) \text{ is convex },~\forall x\\
	[2]~&f(y)\le f(x)+\nabla f(x)^T(y-x)+\frac{L}{2}\lVert y-x\rVert^2,~\forall x,y.\\
	[3]~&(\nabla f(x) - \nabla f(y)^T(x-y) \le L \rVert x-y\rVert^2, ~\forall x,y.\\
	[4]~&f(\alpha x+ (1-\alpha) y) \ge \alpha f(x) + (1-\alpha) f(y) - \frac{\alpha (1-\alpha)L}{2}\lVert x-y\rVert^2,~\forall x,y \text{ and } \alpha \in [0,1]\\
	[5]~&f(y)\ge f(x)+\nabla f(x)^T(y-x)+\frac{1}{2L}\lVert\nabla f(y)-\nabla f(x)\rVert^2,~\forall x,y.\\
	[6]~&(\nabla f(x) - \nabla f(y)^T(x-y) \ge \frac{1}{L} \lVert \nabla f(x)-\nabla f(y)\rVert^2, ~\forall x,y.\\
	[7]~&f(\alpha x+ (1-\alpha) y) \le \alpha f(x) + (1-\alpha) f(y) - \frac{\alpha (1-\alpha)}{2L}\lVert\nabla f(x)-\nabla f(y)\rVert^2,~\forall x,y \text{ and }\alpha \in [0,1].
\end{align}
$$

**Note:** We assume that the domain for $$f$$ and $$g$$ are both $$\mathbb{R}^n$$, and hence convex.

<div class="divider"></div>

### Relationships Between Conditions

The next proposition gives the relationships between all the conditions mentioned above. If you have already mastered all the tricks in the post of [strong convexity](http://xingyuzhou.org/blog/notes/strong-convexity), you can easily prove all the results by yourself. Try it now!

>**Proposition** _For a function $$f$$ with a Lipschitz continuous gradient over $$\mathbb{R}^n$$, the following implications hold:_
>
$$
[5] \equiv [7] \rightarrow [6] \rightarrow [0] \rightarrow [1] \equiv [2] \equiv [3] \equiv [4]
$$
>
If we further assume that $$f$$ is convex, then we have all the conditions $$[0]-[7]$$ are equivalent.

_Proof:_ Again, the key idea behind the proof is transformation, i.e., transform a $$f$$ with Lipschitz continuous gradient to another convex function $$g$$, which enables us to apply the equivalent conditions for convexity.

$$[1] \equiv [2]$$: It follows from the first-order condition for convexity of $$g(x)$$, i.e., $$g(x)$$ is convex if and only if $$g(y)\ge g(x) + \nabla g(x)^T(y-x),~\forall x,y.$$

$$[1] \equiv [3]$$: It follows from the monotone gradient condition for convexity of $$g(x)$$, i.e., $$g(x)$$ is convex if and only if $$(\nabla g(x) - \nabla g(y))^T(x-y) \ge 0,~\forall x,y.$$

$$[1] \equiv [4]$$: It simply follows from the definition of convexity, i.e., $$g(x)$$ is convex if $$g(\alpha x+ (1-\alpha) y) \le \alpha g(x) + (1-\alpha) g(y), ~\forall x,y, \alpha\in [0,1].$$

$$[0]\rightarrow[3]$$: It simply follows from the Cauchy-Schwartz inequality.

$$[6]\rightarrow[0]$$: It simply follows from the Cauchy-Schwartz inequality.

$$[7]\rightarrow[5]$$: Interchanging $$x$$ and $$y$$ in [7] and re-arranging, we have

$$
\begin{align}
	f(y) \ge f(x) + \frac{f(x+\alpha (y-x)) -f(x)}{\alpha} + \frac{1-\alpha}{2L}\lVert\nabla f(x) - \nabla f(y)\rVert^2
\end{align}
$$

As $$\alpha \downarrow 0$$, we get $$[5]$$.

$$[5]\rightarrow[7]$$: Let $$z = \alpha x + (1-\alpha) y \in \mathbb{R}^n$$, we have

$$
	f(x)\ge f(z)+\nabla f(z)^T(x-z)+\frac{1}{2L}\lVert \nabla f(x)-\nabla f(z)\rVert^2
$$

$$
	f(y)\ge f(z)+\nabla f(z)^T(y-z)+\frac{1}{2L}||\nabla f(y)-\nabla f(z)||^2
$$

Multiplying the first inequality with $$\alpha$$ and second inequality with $$1-\alpha$$, and adding them together yields

$$
\begin{align}
	f(\alpha x+ (1-\alpha) y) &\le \alpha f(x) + (1-\alpha) f(y) - \frac{\alpha}{2L}||\nabla f(x)-\nabla f(z)||^2 - \frac{1-\alpha}{2L}||\nabla f(y)-\nabla f(z)||^2\\
	& \le \alpha f(x) + (1-\alpha) f(y) - \frac{\alpha (1-\alpha)}{2L}||\nabla f(x)-\nabla f(y)||^2
\end{align}
$$

where the second inequality follows from the inequality $$\alpha \lVert x\rVert^2 + (1-\alpha) \lVert y\rVert^2 \ge \alpha (1-\alpha)\lVert x-y\rVert^2.$$

If $$f$$ is convex, we can easily show $$[1] \rightarrow [5]$$, which implies that all the conditions are equivalent in this case.

$$[1]\rightarrow[5]$$: Let us consider the function $$\phi_x(z) = f(z) - \nabla f(x)^T z$$, which obtain its optimum at $$z^* = x$$ as $$f$$ is convex. Moreover, we have $$h(z) = \frac{L}{2}z^Tz - \phi_x(z)$$ is convex since $$[1]$$ holds, which implies that

$$
\phi_x(z)\le \phi_x(y)+\nabla \phi_x(y)^T(z-y)+\frac{L}{2}\lVert z-y\rVert^2
$$
Taking minimization with respect to $$z$$ on both sides, yields,

$$
\begin{align}
	f(y) - f(x) - \nabla f(x) (y-x) &= \phi_x(y) - \phi_x(x)\\
	& \ge \frac{1}{2L}\lVert \nabla \phi_x(y)\rVert^2\\
	& = \frac{1}{2L}\lVert \nabla f(y) - \nabla f(x)\rVert^2
\end{align}
$$

Re-arranging gives the result. $$\tag*{$\Box$}$$ -->

<!-- ### Citation
Recently, I have received a lot of emails from my dear readers that inquire about how to cite the content in my blog. I am quite surprised and also glad that my blog posts are more welcome than expected. Fortunately, I have an arXiv paper that summarizes all the results. Here is the citation form:

> Zhou, Xingyu. "On the Fenchel Duality between Strong Convexity and Lipschitz Continuous Gradient." arXiv preprint arXiv:1803.06573 (2018). -->

<p class="center"><strong>THE END</strong></p>

<div class="divider"></div>

<p>Now, it‚Äôs time to take a break by appreciating the masterpiece of Monet.</p>

<p class="center"><img src="../assets/post_images/cliff.jpg" alt="Monet" height="400px" width="500px" /></p>

<p class="center"><strong>Cliff Walk at Pourville</strong></p>
<p class="center"><em>courtesy of www.Claude-Monet.com</em></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In this paper, we follow Def.1, though the same guarantee holds under Def.2 by adaptive composition. Will update it.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET